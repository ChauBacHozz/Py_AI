{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bored-updating",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prime-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-craps",
   "metadata": {},
   "source": [
    "# Importing Datasets\n",
    "\n",
    "I have already imported the dataset. If you want to use a different dataset, then you can give its path down below.\n",
    "\n",
    "The shape of X should be (n, m),\n",
    "And that of Y should be (10, m)\n",
    "\n",
    "Here, \n",
    "- n = total number of features\n",
    "- m = total number of observations in our dataset\n",
    "- 10 = number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accredited-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"MNIST/train_X.csv\").T\n",
    "Y_train = pd.read_csv(\"MNIST/train_label.csv\").T\n",
    "\n",
    "X_test = pd.read_csv(\"MNIST/test_X.csv\").T\n",
    "Y_test = pd.read_csv(\"MNIST/test_label.csv\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train :  (784, 999)\n",
      "shape of Y_train :  (10, 999)\n",
      "shape of X_test :  (784, 349)\n",
      "shape of Y_test :  (10, 349)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of X_train : \", X_train.shape)\n",
    "print(\"shape of Y_train : \", Y_train.shape)\n",
    "\n",
    "print(\"shape of X_test : \", X_test.shape)\n",
    "print(\"shape of Y_test : \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-association",
   "metadata": {},
   "source": [
    "### Visualizing our Dataset\n",
    "\n",
    "Our dataset is in the form of numbers. So, to visualize it properly, we will display it in the form of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excess-expression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = int(random.randrange(0,X_train.shape[1]))\n",
    "plt.imshow(X_train.iloc[:, index].values.reshape((28,28)),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-graphic",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Initialize parameters Randomly\n",
    "$ W_1 = np.random.randn(n_1, n_0) $\n",
    "\n",
    "$ b_1 = np.zeros((n_1, 1))$\n",
    "\n",
    "$ W_2 = np.random.randn(n_2, n_1) $\n",
    "\n",
    "$ b_2 = np.zeros((n_2, 1))$\n",
    "\n",
    "\n",
    "## *Repeat Below Steps for many times : *\n",
    "\n",
    "\n",
    "## Forward Propagation \n",
    "\n",
    "$ Z_1 = W_1 * X + B_1 $\n",
    "\n",
    "$ A_1 = f ( Z_1 ) $  \n",
    "\n",
    "$ Z_2 = W2 * A_1 + B_2 $\n",
    "\n",
    "$ A_2 = Softmax( Z_2 ) $\n",
    "\n",
    "## Softmax \n",
    "\n",
    "$ a_i = \\frac{e^{z_i}}{\\sum_{i=k}^ne^{z_k}}$\n",
    "\n",
    "\n",
    "## Cost Function \n",
    "\n",
    "$Loss = - \\sum_{i=k}^{n}[ y_k*log(a_k) ]$\n",
    "\n",
    "$Cost = - \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n}[ y_k*log(a_k) ]$\n",
    "\n",
    "\n",
    "\n",
    "## Backward Propagation\n",
    "$dZ_2 = ( A_2 - Y )$\n",
    "\n",
    "$ dW_2 = \\frac{1}{m}. dZ_2 . A_1^T$\n",
    "\n",
    "$ dB_2 = \\frac{1}{m}.sum(dZ_2, 1)$\n",
    "\n",
    "\n",
    "\n",
    "$dZ_1 = W_2^T . dZ_2 * f_1^|(Z_1) $\n",
    "\n",
    "$dW_1 = \\frac{1}{m}.dZ_1.X^T$\n",
    "\n",
    "$dB_1 = \\frac{1}{m}.sum(dZ_1, 1)$\n",
    "\n",
    "\n",
    "## Updating Parameters\n",
    "\n",
    "$ W_2 = W_2 -  \\alpha * \\frac{\\partial Cost }{\\partial W_2}$ \n",
    "\n",
    "$ B_2 = B_2 -  \\alpha * \\frac{\\partial Cost }{\\partial B_2}$ \n",
    "\n",
    "$ W_1 = W_1 -  \\alpha * \\frac{\\partial Cost }{\\partial W_1}$ \n",
    "\n",
    "$ B_1 = B_1 -  \\alpha * \\frac{\\partial Cost }{\\partial B_1}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-seven",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "***Now, its your time to implement !***\n",
    "\n",
    "Complete the below functions for Activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "better-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    ## Your Code Here ##\n",
    "    return np.tanh(x)\n",
    "    ## Code Ends ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "immune-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    ## Your Code Here ##\n",
    "    return np.maximum(0, x)\n",
    "    ## Code Ends ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "several-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    ## Your Code Here ##\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis = 0)\n",
    "    ## Code Ends ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-gambling",
   "metadata": {},
   "source": [
    "The function *derivative_tanh* must return the derivative of tanh.\n",
    "The function *derivative_relu* must return the derivative of ReLU\n",
    "\n",
    "\n",
    "Also note, I made a small mistake in video. derivative of tanh is given by 1 - tanh^2(x).\n",
    "\n",
    "\n",
    "so, derivative_tanh(x):\n",
    "\n",
    "return 1 - np.power(np.tanh(x), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "german-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_tanh(x):\n",
    "    ## Your Code Here ##\n",
    "    return 1 - np.power(np.tanh(x), 2)\n",
    "    ## Code Ends ##\n",
    "\n",
    "def derivative_relu(x):\n",
    "    ## Your Code Here ##\n",
    "    return np.array(x > 0, dtype = np.float32)\n",
    "    ## Code Ends ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-heading",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "\n",
    "We need to initialize the **W** parameters randomly, and **B** with zeros\n",
    "\n",
    "- np.random.randn(a, b) return a numpy array of shape(a, b) with small random values\n",
    "- For making the values small, we multiply 0.01\n",
    "- np.zeros((a, b) return a numpy array of shape(a, b) with zeros \n",
    "\n",
    "### Why need small weights W?\n",
    "If we initialize weights will large values, then Z = W * X + B, will be large. For functions like tanh and sigmoid, the slope becomes very less for large Z value, thus learning can be very slow.\n",
    "\n",
    "#### Remember, we had an increase in the cost function at the beginning while training the model with ReLU activation function?\n",
    "It is because our weights were still very large and it was creating problem for training our model.\n",
    "\n",
    "Multiply weights with 0.001 instead of 0.01, and you will see that the graph becomes normal, with a smooth decrease in cost value.\n",
    "\n",
    "There are many weight initialization techniques available as well, to solve other such problems. We will learn those in the upcoming videos.\n",
    "\n",
    "Now, We need to return a dictionary containing all the parameters.\n",
    "\n",
    "More about np.random.randn here : https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "several-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    ## Complete the code below ##\n",
    "    w1 = np.random.randn()*0.001\n",
    "    b1 = np.random.randn()*0.001\n",
    "    w2 = np.random.randn()*0.001\n",
    "    b2 = np.random.randn()*0.001\n",
    "    ## Your Code ends ##\n",
    "    \n",
    "    parameters = {\n",
    "        \"w1\" : w1, \n",
    "        \"b1\" : b1, \n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-sauce",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "We need to impletement the following equation for forward propagation :\n",
    "\n",
    "$ Z_1 = W_1 * X + B_1 $\n",
    "\n",
    "$ A_1 = f ( Z_1 ) $  \n",
    "\n",
    "$ Z_2 = W2 * A_1 + B_2 $\n",
    "\n",
    "$ A_2 = Softmax( Z_2 ) $\n",
    "\n",
    "For f(x), you can use either tanh or ReLU activation function. \n",
    "\n",
    "But also use the same for Backpropagation as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "described-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, parameters):\n",
    "    \n",
    "    # To fetch the parameters\n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    ## Complete the Code below : ##\n",
    "    z1 = np.dot(w1, x) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(w2, a1) + b2   \n",
    "    a2 = softmax(z2)\n",
    "    ## Your Code ends ##\n",
    "    \n",
    "    # To return our Zs and As\n",
    "    forward_cache = {\n",
    "        \"z1\" : z1,\n",
    "        \"a1\" : a1,\n",
    "        \"z2\" : z2,\n",
    "        \"a2\" : a2\n",
    "    }\n",
    "    \n",
    "    return forward_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-tourist",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "$Loss = - \\sum_{k=1}^{n}[ y_k*log(a_k) ]$ .. *for 1 observation*\n",
    "\n",
    "$Cost = - \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n}[ y_k*log(a_k) ]$  .. *for all m observations*\n",
    "\n",
    "You need to return the cost in the below function\n",
    "\n",
    "You can use np.sum()\n",
    "- np.sum(A, axis = 1, keepdims = True) return the column-wise sum for a matrix A\n",
    "- np.sum(A, axis = 0, keepdims = True) returns the row-wise sum for a matrix A\n",
    "- np.sum(A) returns the summation of all the elements of A\n",
    "\n",
    "*keepdims = True keeps the dimenstion in place. In certain cases, the returned sum can be of shape(m,) instead of shape(m, 1).\n",
    "So, keepdims = True forces it to return the sum in shape(m, 1) instead of shape(m,)*\n",
    "\n",
    "\n",
    "More about np.sum() here : https://numpy.org/doc/stable/reference/generated/numpy.sum.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "conservative-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(a2, y):\n",
    "    m = a2.shape[1]\n",
    "    ## Your Code Here ##\n",
    "    \n",
    "    Loss = (-1/m)*np.sum(y * np.log(a2))\n",
    "    ## Code Ends ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-draft",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "\n",
    "We need to implement the below equations\n",
    "\n",
    "$dZ_2 = ( A_2 - Y )$\n",
    "\n",
    "$ dW_2 = \\frac{1}{m}. dZ_2 . A_2^T$\n",
    "\n",
    "$ dB_2 = \\frac{1}{m}.sum(dZ_2, 1)$\n",
    "\n",
    "\n",
    "\n",
    "$dZ_1 = W_2^T . dZ_2 * f_1^|(Z_1) $\n",
    "\n",
    "$dW_1 = \\frac{1}{m}.dZ_1.A_1^T$\n",
    "\n",
    "$dB_1 = \\frac{1}{m}.sum(dZ_1, 1)$\n",
    "\n",
    "Helper python functions :\n",
    "- A.T returns the transpose of matrix A\n",
    "- np.dot(A, B) returns the matrix multiplication of A and B\n",
    "- A*B returns the element wise multi-plication for A and B\n",
    "- np.sum(A, axis = 1, keepdims = True) return the column-wise sum for a matrix A\n",
    "- np.sum(A, axis = 0, keepdims = True) returns the row-wise sum for a matrix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "regulation-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(x, y, parameters, forward_cache):\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    \n",
    "    # Fetching our parameters\n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2'] \n",
    "    \n",
    "    # Fetching our forward_cache\n",
    "    a1 = forward_cache['a1']\n",
    "    a2 = forward_cache['a2']\n",
    "    \n",
    "    \n",
    "    ## Complete the Code below ##\n",
    "    dz2 = a2 - y\n",
    "    dw2 = (1/m) * np.dot(dz2, )\n",
    "    db2 = (1/m) * (dz2 + 1)\n",
    "    \n",
    "    dz1 = \n",
    "    dw1 = \n",
    "    db1 = \n",
    "    ## Your Code ends ##\n",
    "    \n",
    "    ## Returning the Gradients\n",
    "    gradients = {\n",
    "        \"dw1\" : dw1,\n",
    "        \"db1\" : db1,\n",
    "        \"dw2\" : dw2, \n",
    "        \"db2\" : db2\n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-sentence",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "convertible-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \n",
    "    # Fetching our parameters\n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2'] \n",
    "    \n",
    "    # Fetching our gradients\n",
    "    dw1 = gradients['dw1']\n",
    "    db1 = gradients['db1']\n",
    "    dw2 = gradients['dw2']\n",
    "    db2 = gradients['db2']\n",
    "    \n",
    "    ## Complete the Code below ##\n",
    "    w1 = \n",
    "    b1 = \n",
    "    w2 = \n",
    "    b2 = \n",
    "    ## Your code ends ##\n",
    "    \n",
    "    # Returning the updated parameters\n",
    "    Parameters = {\n",
    "        \"w1\" : w1, \n",
    "        \"b1\" : b1, \n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2\n",
    "    }\n",
    "    \n",
    "    return Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-effort",
   "metadata": {},
   "source": [
    "# Complete Model\n",
    "\n",
    "Implement the entire Neural Network here\n",
    "\n",
    "### Instructions :\n",
    "\n",
    "We need to initialize parameters once, and after that, we will run the following in a loop:\n",
    "- forward_prop(x, parameters)\n",
    "- cost_function(a2, y)\n",
    "- backward_prop(x, y, parameters, forward_cache)\n",
    "- parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "### Return :\n",
    "- parameters, which will be our trained parameters\n",
    "- cost_list, which contains cost for every iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "powerful-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y, n_h, learning_rate, iterations):\n",
    "    \n",
    "    ## Complete the Code Below ##\n",
    "    n_x =                        # must return the number of neurons/features in input layer\n",
    "    n_y =                        # must return the number of neurons in output layer\n",
    "    ## Your Code ends ##\n",
    "    \n",
    "    cost_list = []\n",
    "    \n",
    "    ## Complete the Code Below ##\n",
    "    parameters = \n",
    "    ## Your Code ends ##\n",
    "    \n",
    "    ## Complete the Code Below ##\n",
    "    for i in range(          ):\n",
    "        \n",
    "        # Forward Propagation\n",
    "     \n",
    "        # Cost Function\n",
    "        \n",
    "        # Backward propagation\n",
    "        \n",
    "        # Update Parameters\n",
    "        \n",
    "        cost_list.append(    )\n",
    "        \n",
    "        if i%(iterations/10) == 0 :\n",
    "            print(\"cost after\", i, \"iters is\" ,     cost   )\n",
    "            \n",
    "            \n",
    "    ## Your Code ends ##\n",
    "    \n",
    "    \n",
    "    return parameters, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete the Code Below ##\n",
    "\n",
    "n_h = \n",
    "learning_rate = \n",
    "iterations = \n",
    "\n",
    "## Your Code ends ##\n",
    "\n",
    "Parameters, Cost_list = model(X, Y, n_h = n_h, learning_rate = learning_rate, iterations = iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, iterations+1)\n",
    "plt.plot(t, Cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-contribution",
   "metadata": {},
   "source": [
    "# Checking Accuracy\n",
    "\n",
    "Run the below cells to check your model Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(inp, labels, parameters):\n",
    "    forward_cache = forward_prop(inp, parameters)\n",
    "    a_out = forward_cache['a2']   # containes propabilities with shape(10, 1)\n",
    "    \n",
    "    a_out = np.argmax(a_out, 0)  # 0 represents row wise \n",
    "    \n",
    "    labels = np.argmax(labels, 0)\n",
    "    \n",
    "    acc = np.mean(a_out == labels)*100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Train Dataset\", accuracy(X_train, Y_train, Parameters), \"%\")\n",
    "print(\"Accuracy of Test Dataset\", round(accuracy(X_test, Y_test, Parameters), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(random.randrange(0,X_test.shape[1]))\n",
    "plt.imshow(X_test[:, idx].reshape((28,28)),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "cache = forward_prop(X_test[:, idx].reshape(X_test[:, idx].shape[0], 1), Parameters)\n",
    "a_pred = cache['a2']  \n",
    "a_pred = np.argmax(a_pred, 0)\n",
    "\n",
    "print(\"Our model says it is :\", a_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-submission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1658d7c139ecb4375ab76a4a9bda7b71499aefd554bc284ea3c47687afb5394a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
